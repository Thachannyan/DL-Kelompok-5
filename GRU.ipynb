{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import librosa.display\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH ='Dataset/genres_original/'\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 30\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mfcc(dataset_path  , n_mfcc = 13 , n_fft = 2048 , hop_length = 512 , num_segments = 5):\n",
    "       \"\"\"Extracts MFCCs from music dataset and saves them into variabel data\n",
    "        :param dataset_path (str): Path to dataset\n",
    "        :param num_mfcc (int): Number of coefficients to extract\n",
    "        :param n_fft (int): Interval we consider to apply FFT. Measured in # of samples\n",
    "        :param hop_length (int): Sliding window for FFT. Measured in # of samples\n",
    "        :param: num_segments (int): Number of segments we want to divide sample tracks into\n",
    "     \n",
    "        \"\"\"\n",
    "    data = {\n",
    "        # dictionary to store mapping, labels, and MFCCs\n",
    "        'mapping' :[] ,\n",
    "        'mfcc' :[] ,\n",
    "        'labels' : []\n",
    "    }\n",
    "    num_sample_per_segment = int(SAMPLES_PER_TRACK / num_segments) \n",
    "    expected_num_mfcc_vector_per_segment = math.ceil(num_sample_per_segment / hop_length)\n",
    "    print(f'{expected_num_mfcc_vector_per_segment} that is the length of the sequence')\n",
    "    \n",
    "    # dictionary to store mapping, labels, and MFCCs\n",
    "    for i , (dirpath , dirnames , filenames) in enumerate(os.walk(dataset_path)):\n",
    "        \n",
    "        # ensure we're processing a genre sub-folder level\n",
    "        if dirpath != dataset_path:\n",
    "            \n",
    "            # save genre label (i.e., sub-folder name) in the mapping\n",
    "            semantic_label = dirpath.split('/')[-1]\n",
    "            data['mapping'].append(semantic_label)\n",
    "            \n",
    "            # process all audio files in genre sub-dir\n",
    "            for file in filenames:\n",
    "                \n",
    "                # load audio file\n",
    "                file_path = os.path.join(dirpath , file)\n",
    "                try :\n",
    "                    signal , sr = librosa.load(file_path , sr = SAMPLE_RATE)\n",
    "                    \n",
    "                    # process all segments of audio file\n",
    "                    for s in range(num_segments):\n",
    "                        \n",
    "                        # calculate start and finish sample for current segment\n",
    "                        start_sample = num_sample_per_segment * s\n",
    "                        finish_sample = start_sample + num_sample_per_segment\n",
    "                        \n",
    "                        # extract mfcc\n",
    "                        mfcc = librosa.feature.mfcc(y = signal[start_sample:finish_sample] , sr = SAMPLE_RATE , \n",
    "                                                   n_mfcc=13 , n_fft = n_fft , hop_length = hop_length)\n",
    "                        mfcc = mfcc.T\n",
    "                        \n",
    "                        # store only mfcc feature with expected number of vectors\n",
    "                        if len(mfcc) == expected_num_mfcc_vector_per_segment:\n",
    "                            data['mfcc'].append(mfcc.tolist())\n",
    "                            data['labels'].append(i-1)\n",
    "                except:\n",
    "                    pass\n",
    "                        \n",
    "            print(f\"{dirpath.split('/')[-1]} is loaded successfully\")\n",
    "                        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 that is the length of the sequence\n",
      "blues is loaded successfully\n",
      "classical is loaded successfully\n",
      "country is loaded successfully\n",
      "disco is loaded successfully\n",
      "hiphop is loaded successfully\n",
      "jazz is loaded successfully\n",
      "jazz\\.ipynb_checkpoints is loaded successfully\n",
      "metal is loaded successfully\n",
      "pop is loaded successfully\n",
      "reggae is loaded successfully\n",
      "rock is loaded successfully\n"
     ]
    }
   ],
   "source": [
    "data_dict = save_mfcc(DATASET_PATH  , num_segments=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data_dict['mfcc'])\n",
    "label = np.array(data_dict['labels']).reshape(-1 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9996, 130, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9996, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, validation and test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_val, X_test, y_train_val, y_test =train_test_split(data , label  ,test_size=0.2 ,shuffle= True)\n",
    "X_train, X_valid, y_train, y_valid =train_test_split(X_train_val , y_train_val  ,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build network topology\n",
    "model = keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, 13]),\n",
    "    keras.layers.LayerNormalization(),\n",
    "    keras.layers.GRU(64, return_sequences=True),\n",
    "    keras.layers.LayerNormalization(),\n",
    "    keras.layers.GRU(32),\n",
    "    \n",
    "    keras.layers.Dense(11 ,  activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss ='sparse_categorical_crossentropy', optimizer = keras.optimizers.Adam() , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "800/800 [==============================] - 126s 158ms/step - loss: 1.4426 - accuracy: 0.4819 - val_loss: 1.2033 - val_accuracy: 0.5775\n",
      "Epoch 2/30\n",
      "800/800 [==============================] - 114s 142ms/step - loss: 1.0368 - accuracy: 0.6382 - val_loss: 0.9920 - val_accuracy: 0.6450\n",
      "Epoch 3/30\n",
      "800/800 [==============================] - 108s 135ms/step - loss: 0.8559 - accuracy: 0.7004 - val_loss: 0.9020 - val_accuracy: 0.6812\n",
      "Epoch 4/30\n",
      "800/800 [==============================] - 107s 133ms/step - loss: 0.7205 - accuracy: 0.7516 - val_loss: 0.7600 - val_accuracy: 0.7244\n",
      "Epoch 5/30\n",
      "800/800 [==============================] - 105s 131ms/step - loss: 0.6151 - accuracy: 0.7921 - val_loss: 0.7536 - val_accuracy: 0.7362\n",
      "Epoch 6/30\n",
      "800/800 [==============================] - 105s 132ms/step - loss: 0.5205 - accuracy: 0.8213 - val_loss: 0.7119 - val_accuracy: 0.7606\n",
      "Epoch 7/30\n",
      "800/800 [==============================] - 105s 132ms/step - loss: 0.4672 - accuracy: 0.8430 - val_loss: 0.6150 - val_accuracy: 0.7937\n",
      "Epoch 8/30\n",
      "800/800 [==============================] - 107s 134ms/step - loss: 0.4073 - accuracy: 0.8616 - val_loss: 0.5684 - val_accuracy: 0.8075\n",
      "Epoch 9/30\n",
      "800/800 [==============================] - 119s 149ms/step - loss: 0.3573 - accuracy: 0.8857 - val_loss: 0.6495 - val_accuracy: 0.7837\n",
      "Epoch 10/30\n",
      "800/800 [==============================] - 117s 147ms/step - loss: 0.3309 - accuracy: 0.8895 - val_loss: 0.6127 - val_accuracy: 0.7869\n",
      "Epoch 11/30\n",
      "800/800 [==============================] - 115s 143ms/step - loss: 0.2882 - accuracy: 0.9046 - val_loss: 0.5880 - val_accuracy: 0.8125\n",
      "Epoch 12/30\n",
      "800/800 [==============================] - 106s 132ms/step - loss: 0.2670 - accuracy: 0.9090 - val_loss: 0.6058 - val_accuracy: 0.8138\n",
      "Epoch 13/30\n",
      "800/800 [==============================] - 101s 127ms/step - loss: 0.2567 - accuracy: 0.9165 - val_loss: 0.5873 - val_accuracy: 0.8106\n",
      "Epoch 14/30\n",
      "800/800 [==============================] - 108s 135ms/step - loss: 0.2362 - accuracy: 0.9220 - val_loss: 0.5668 - val_accuracy: 0.8175\n",
      "Epoch 15/30\n",
      "800/800 [==============================] - 108s 135ms/step - loss: 0.2122 - accuracy: 0.9256 - val_loss: 0.5737 - val_accuracy: 0.8294\n",
      "Epoch 16/30\n",
      "800/800 [==============================] - 103s 128ms/step - loss: 0.2257 - accuracy: 0.9207 - val_loss: 0.6218 - val_accuracy: 0.8037\n",
      "Epoch 17/30\n",
      "800/800 [==============================] - 105s 132ms/step - loss: 0.2113 - accuracy: 0.9290 - val_loss: 0.6401 - val_accuracy: 0.8131\n",
      "Epoch 18/30\n",
      "800/800 [==============================] - 108s 136ms/step - loss: 0.1857 - accuracy: 0.9381 - val_loss: 0.6384 - val_accuracy: 0.8194\n",
      "Epoch 19/30\n",
      "800/800 [==============================] - 107s 134ms/step - loss: 0.1818 - accuracy: 0.9386 - val_loss: 0.6288 - val_accuracy: 0.8169\n",
      "Epoch 20/30\n",
      "800/800 [==============================] - 104s 130ms/step - loss: 0.1623 - accuracy: 0.9443 - val_loss: 0.6793 - val_accuracy: 0.8037\n",
      "Epoch 21/30\n",
      "800/800 [==============================] - 105s 131ms/step - loss: 0.1722 - accuracy: 0.9418 - val_loss: 0.6428 - val_accuracy: 0.8156\n",
      "Epoch 22/30\n",
      "800/800 [==============================] - 104s 130ms/step - loss: 0.1373 - accuracy: 0.9512 - val_loss: 0.7026 - val_accuracy: 0.8075\n",
      "Epoch 23/30\n",
      "800/800 [==============================] - 105s 131ms/step - loss: 0.1401 - accuracy: 0.9523 - val_loss: 0.6385 - val_accuracy: 0.8200\n",
      "Epoch 24/30\n",
      "800/800 [==============================] - 103s 129ms/step - loss: 0.1792 - accuracy: 0.9362 - val_loss: 0.6676 - val_accuracy: 0.8094\n",
      "Epoch 25/30\n",
      "800/800 [==============================] - 106s 132ms/step - loss: 0.1459 - accuracy: 0.9501 - val_loss: 0.6223 - val_accuracy: 0.8275\n",
      "Epoch 26/30\n",
      "800/800 [==============================] - 108s 135ms/step - loss: 0.1201 - accuracy: 0.9589 - val_loss: 0.6864 - val_accuracy: 0.8169\n",
      "Epoch 27/30\n",
      "800/800 [==============================] - 107s 134ms/step - loss: 0.1382 - accuracy: 0.9533 - val_loss: 0.6184 - val_accuracy: 0.8275\n",
      "Epoch 28/30\n",
      "800/800 [==============================] - 108s 135ms/step - loss: 0.1236 - accuracy: 0.9589 - val_loss: 0.6582 - val_accuracy: 0.8087\n",
      "Epoch 29/30\n",
      "800/800 [==============================] - 107s 134ms/step - loss: 0.1168 - accuracy: 0.9609 - val_loss: 0.6677 - val_accuracy: 0.8200\n",
      "Epoch 30/30\n",
      "800/800 [==============================] - 108s 135ms/step - loss: 0.1300 - accuracy: 0.9551 - val_loss: 0.8051 - val_accuracy: 0.7881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x202354ce7f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train , y_train ,validation_data=(X_valid , y_valid) , epochs=30 , batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Music_genre_classification_GRU.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
