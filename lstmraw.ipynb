{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import librosa.display\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH ='Dataset/genres_original/'\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 30\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mfcc(dataset_path  , n_mfcc = 13 , n_fft = 2048 , hop_length = 512 , num_segments = 5):\n",
    "    data = {\n",
    "        'mapping' :[] ,\n",
    "        'mfcc' :[] ,\n",
    "        'labels' : []\n",
    "    }\n",
    "    num_sample_per_segment = int(SAMPLES_PER_TRACK / num_segments) #so here every 30 second we would split then into 5 segments\n",
    "    expected_num_mfcc_vector_per_segment = math.ceil(num_sample_per_segment / hop_length)\n",
    "    print(f'{expected_num_mfcc_vector_per_segment} that is the length of the sequence')\n",
    "    \n",
    "    \n",
    "    for i , (dirpath , dirnames , filenames) in enumerate(os.walk(dataset_path)):\n",
    "        \n",
    "        \n",
    "        if dirpath != dataset_path:\n",
    "            semantic_label = dirpath.split('/')[-1]\n",
    "            data['mapping'].append(semantic_label)\n",
    "            for file in filenames:\n",
    "                file_path = os.path.join(dirpath , file)\n",
    "                try :\n",
    "                    signal , sr = librosa.load(file_path , sr = SAMPLE_RATE)\n",
    "                    for s in range(num_segments):\n",
    "                        start_sample = num_sample_per_segment * s\n",
    "                        finish_sample = start_sample + num_sample_per_segment\n",
    "                        mfcc = librosa.feature.mfcc(y = signal[start_sample:finish_sample] , sr = SAMPLE_RATE , \n",
    "                                                   n_mfcc=13 , n_fft = n_fft , hop_length = hop_length)\n",
    "                        mfcc = mfcc.T # as we have the data (sequence number , number of features)\n",
    "                        if len(mfcc) == expected_num_mfcc_vector_per_segment:\n",
    "                            data['mfcc'].append(mfcc.tolist())\n",
    "                            data['labels'].append(i-1)\n",
    "                except:\n",
    "                    pass\n",
    "                        \n",
    "            print(f\"{dirpath.split('/')[-1]} is loaded successfully\")\n",
    "                        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 that is the length of the sequence\n",
      "blues is loaded successfully\n",
      "classical is loaded successfully\n",
      "country is loaded successfully\n",
      "disco is loaded successfully\n",
      "hiphop is loaded successfully\n",
      "jazz is loaded successfully\n",
      "jazz\\.ipynb_checkpoints is loaded successfully\n",
      "metal is loaded successfully\n",
      "pop is loaded successfully\n",
      "reggae is loaded successfully\n",
      "rock is loaded successfully\n"
     ]
    }
   ],
   "source": [
    "data_dict = save_mfcc(DATASET_PATH  , num_segments=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data_dict['mfcc'])\n",
    "label = np.array(data_dict['labels']).reshape(-1 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9996, 130, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9996, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_val, X_test, y_train_val, y_test =train_test_split(data , label  ,test_size=0.2 ,shuffle= True)\n",
    "X_train, X_valid, y_train, y_valid =train_test_split(X_train_val , y_train_val  ,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.LSTM(128, return_sequences=True, input_shape=[None, 13]),\n",
    "    keras.layers.LayerNormalization(),\n",
    "    keras.layers.LSTM(64, return_sequences=True),\n",
    "    keras.layers.LayerNormalization(),\n",
    "    keras.layers.LSTM(32),\n",
    "    keras.layers.Dense(11 ,  activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss ='sparse_categorical_crossentropy', optimizer = keras.optimizers.Adam() , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "800/800 [==============================] - 94s 118ms/step - loss: 1.6805 - accuracy: 0.4023 - val_loss: 1.4765 - val_accuracy: 0.4644\n",
      "Epoch 2/30\n",
      "800/800 [==============================] - 92s 115ms/step - loss: 1.4140 - accuracy: 0.4892 - val_loss: 1.4689 - val_accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "800/800 [==============================] - 92s 115ms/step - loss: 1.3216 - accuracy: 0.5184 - val_loss: 1.3162 - val_accuracy: 0.5194\n",
      "Epoch 4/30\n",
      "800/800 [==============================] - 92s 115ms/step - loss: 1.2571 - accuracy: 0.5478 - val_loss: 1.3480 - val_accuracy: 0.5031\n",
      "Epoch 5/30\n",
      "800/800 [==============================] - 92s 115ms/step - loss: 1.1934 - accuracy: 0.5705 - val_loss: 1.3086 - val_accuracy: 0.5319\n",
      "Epoch 6/30\n",
      "800/800 [==============================] - 92s 115ms/step - loss: 1.1345 - accuracy: 0.5987 - val_loss: 1.2168 - val_accuracy: 0.5706\n",
      "Epoch 7/30\n",
      "800/800 [==============================] - 92s 115ms/step - loss: 1.0725 - accuracy: 0.6201 - val_loss: 1.1632 - val_accuracy: 0.5906\n",
      "Epoch 8/30\n",
      "800/800 [==============================] - 90s 113ms/step - loss: 0.9697 - accuracy: 0.6588 - val_loss: 1.1118 - val_accuracy: 0.6019\n",
      "Epoch 9/30\n",
      "800/800 [==============================] - 90s 112ms/step - loss: 0.9111 - accuracy: 0.6831 - val_loss: 1.0393 - val_accuracy: 0.6356\n",
      "Epoch 10/30\n",
      "800/800 [==============================] - 89s 112ms/step - loss: 0.8193 - accuracy: 0.7178 - val_loss: 0.9859 - val_accuracy: 0.6662\n",
      "Epoch 11/30\n",
      "800/800 [==============================] - 91s 114ms/step - loss: 0.8100 - accuracy: 0.7167 - val_loss: 0.9462 - val_accuracy: 0.6650\n",
      "Epoch 12/30\n",
      "800/800 [==============================] - 91s 114ms/step - loss: 0.7530 - accuracy: 0.7369 - val_loss: 0.9268 - val_accuracy: 0.6888\n",
      "Epoch 13/30\n",
      "800/800 [==============================] - 91s 114ms/step - loss: 0.6826 - accuracy: 0.7672 - val_loss: 0.8824 - val_accuracy: 0.7138\n",
      "Epoch 14/30\n",
      "800/800 [==============================] - 92s 115ms/step - loss: 0.6412 - accuracy: 0.7780 - val_loss: 0.8444 - val_accuracy: 0.7138\n",
      "Epoch 15/30\n",
      "800/800 [==============================] - 92s 115ms/step - loss: 0.5845 - accuracy: 0.7994 - val_loss: 0.8966 - val_accuracy: 0.6981\n",
      "Epoch 16/30\n",
      "800/800 [==============================] - 91s 114ms/step - loss: 0.5570 - accuracy: 0.8102 - val_loss: 0.8611 - val_accuracy: 0.7069\n",
      "Epoch 17/30\n",
      "800/800 [==============================] - 91s 114ms/step - loss: 0.5299 - accuracy: 0.8139 - val_loss: 0.9259 - val_accuracy: 0.6975\n",
      "Epoch 18/30\n",
      "800/800 [==============================] - 90s 112ms/step - loss: 0.5070 - accuracy: 0.8294 - val_loss: 0.8550 - val_accuracy: 0.7319\n",
      "Epoch 19/30\n",
      "800/800 [==============================] - 89s 112ms/step - loss: 0.4467 - accuracy: 0.8483 - val_loss: 0.8228 - val_accuracy: 0.7425\n",
      "Epoch 20/30\n",
      "800/800 [==============================] - 89s 112ms/step - loss: 0.4452 - accuracy: 0.8507 - val_loss: 0.8061 - val_accuracy: 0.7356\n",
      "Epoch 21/30\n",
      "800/800 [==============================] - 89s 112ms/step - loss: 0.4267 - accuracy: 0.8552 - val_loss: 0.8426 - val_accuracy: 0.7350\n",
      "Epoch 22/30\n",
      "800/800 [==============================] - 89s 112ms/step - loss: 0.3988 - accuracy: 0.8665 - val_loss: 0.8522 - val_accuracy: 0.7375\n",
      "Epoch 23/30\n",
      "800/800 [==============================] - 89s 112ms/step - loss: 0.3776 - accuracy: 0.8688 - val_loss: 0.7912 - val_accuracy: 0.7513\n",
      "Epoch 24/30\n",
      "800/800 [==============================] - 89s 112ms/step - loss: 0.3637 - accuracy: 0.8771 - val_loss: 0.7875 - val_accuracy: 0.7719\n",
      "Epoch 25/30\n",
      "800/800 [==============================] - 89s 112ms/step - loss: 0.3477 - accuracy: 0.8813 - val_loss: 0.7642 - val_accuracy: 0.7613\n",
      "Epoch 26/30\n",
      "800/800 [==============================] - 89s 111ms/step - loss: 0.3159 - accuracy: 0.8931 - val_loss: 0.7848 - val_accuracy: 0.7638\n",
      "Epoch 27/30\n",
      "800/800 [==============================] - 89s 112ms/step - loss: 0.3120 - accuracy: 0.8963 - val_loss: 0.7934 - val_accuracy: 0.7631\n",
      "Epoch 28/30\n",
      "800/800 [==============================] - 89s 112ms/step - loss: 0.2754 - accuracy: 0.9092 - val_loss: 0.8262 - val_accuracy: 0.7469\n",
      "Epoch 29/30\n",
      "800/800 [==============================] - 90s 112ms/step - loss: 0.2639 - accuracy: 0.9131 - val_loss: 0.8402 - val_accuracy: 0.7594\n",
      "Epoch 30/30\n",
      "800/800 [==============================] - 89s 112ms/step - loss: 0.2575 - accuracy: 0.9142 - val_loss: 0.7436 - val_accuracy: 0.7819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a41db3e220>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train , y_train ,validation_data=(X_valid , y_valid) , epochs=30 , batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 3s 55ms/step - loss: 0.6615 - accuracy: 0.7965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6615481376647949, 0.796500027179718]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test ,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
